# Medical Research Summarizer 

A *local, offline GPT-like assistant* that summarizes and answers questions from uploaded *medical research papers* â€” built entirely with open-source Hugging Face models, no paid APIs or GPU required.

---

## âš™ Overview

This project demonstrates how to build a lightweight *Retrieval-Augmented Generation (RAG)* pipeline on CPU using open-source models.

Users can:
- Upload research PDFs  
- Ask domain-specific questions (e.g., â€œWhat treatment is recommended?â€)  
- Receive concise, citation-linked answers generated locally  

All processing â€” PDF parsing, retrieval, embedding, and summarization â€” happens fully on your machine.

---

## ğŸ§© Architecture

apps/ â”œâ”€â”€ api/ â”‚   â”œâ”€â”€ core/ â”‚   â”‚   â””â”€â”€ config.py        â† Loads global .env and paths â”‚   â”œâ”€â”€ nlp/ â”‚   â”‚   â””â”€â”€ parse_pdf.py     â† Converts PDFs â†’ JSON (sections & sentences) â”‚   â””â”€â”€ rag/ â”‚       â”œâ”€â”€ embed.py         â† Sentence embeddings via MiniLM â”‚       â”œâ”€â”€ retrieve.py      â† Hybrid BM25 + semantic retriever â”‚       â””â”€â”€ generate.py      â† Summarization / answer generation â””â”€â”€ web/ â””â”€â”€ app.py               â† Streamlit web interface

.env / global.env            â† API keys or model names (never commit) requirements.txt             â† Dependencies README.md                    â† This file

---

## ğŸ§  How It Works

### 1ï¸âƒ£ PDF Parsing (parse_pdf.py)
- Extracts text with *PyPDF2* and basic regex cleaning.
- Splits content into structured sections:

```json
{
  "paper_id": "breast-cancer-meta-analysis",
  "sections": [
    {"name": "Abstract", "text": "..."},
    {"name": "Results",  "text": "..."},
    {"name": "Conclusion","text": "..."}
  ]
}

Parsed outputs live in data/parsed/.


---

2ï¸âƒ£ Embedding & Retrieval (embed.py, retrieve.py)

Uses Sentence-Transformers MiniLM-L6-v2 for semantic vectors.

BM25 ranks lexical overlap; MMR ensures diverse top-k evidence.

Combines lexical + semantic + section weighting for better context selection.



---

3ï¸âƒ£ Summarization (generate.py)

Abstractive summarization via DistilBART-CNN (default).

Chunked batching for long texts to stay within CPU limits.

Cleans citations and formats readable paragraphs.

Falls back to extractive summaries if abstractive fails.



---

4ï¸âƒ£ Frontend (app.py)

A Streamlit dashboard providing:

Paper selector

Audience switch (Expert / Patient)

Question box

Instant answer + cited evidence view


Everything executes locally â€” no network calls.


---

ğŸ— Core Concepts

Concept	Explanation

RAG (Retrieval-Augmented Generation)	Combines search + generation for grounded answers.
BM25	Classical keyword ranking (term-frequency / inverse-doc-freq).
Sentence Transformer	Embeds sentences into dense semantic vectors.
MMR	Maximal Marginal Relevance to keep retrieved evidence diverse.
DistilBART	Lightweight summarization transformer ideal for CPUs.



---

ğŸ’¡ Design Philosophy

Educational â†’ transparent end-to-end RAG example

Local-first â†’ no API keys or GPU dependency

Extensible â†’ swap models or add new prompts easily

Transparent â†’ inspect every intermediate step



---

ğŸ“ Workflow Example

1ï¸âƒ£ Parse your PDF

python -m apps.api.nlp.parse_pdf "data/raw/Immunotherapy_Cancer_Treatment.pdf"

Produces data/parsed/immunotherapy_cancer_treatment.json.

2ï¸âƒ£ Run the app

streamlit run apps/web/app.py

3ï¸âƒ£ Use the web UI

Pick a paper ID

Choose audience

Ask questions like â€œWhat are the main findings?â€

View summarized answers + source snippets



---

âš¡ Performance Notes

Metric	Approx Value

CPU latency	5 â€“ 15 s per query
RAM usage	< 1 GB
Disk cache	~500 MB (HF models + embeddings)


Tips to speed up

Reduce top_k in retriever (e.g. 6 â†’ 4)

Lower max_words chunk size in generate.py

Switch to sshleifer/distilbart-cnn-12-6 (smaller model)

Run on GPU if available â†’ set device=0



---

ğŸ§° Extending the App

Goal	File to edit	Hint

Use another summarizer	generate.py	Change model in pipeline()
Tune section weights	retrieve.py	Adjust SECTION_WEIGHTS
Add new prompt style	generate.py	Edit summarize_paper()
Enable GPU	generate.py	Set device=0 in pipeline init
Cache retrieval results	retrieve.py	Decorate search() with lru_cache



---

ğŸ§  Learning Outcomes

By studying this repo youâ€™ll grasp:

How RAG pipelines combine retrieval + generation

Integrating Hugging Face pipelines for CPU inference

Designing transparent, educational AI tools

Structuring research summarization apps end-to-end



---

ğŸ™Œ Authors & Credits

Developed by Sriparno Ganguly (K2E7)
Educational project showcasing open-source RAG for medical literature.

Models from Hugging Face Hub
Example papers from Indian Journal of Medical Research (IJMR).


---

ğŸš€ Quick Start

# clone
git clone https://github.com/<your-username>/medical-research-summarizer
cd medical-research-summarizer

# create venv
python -m venv .venv
.\.venv\Scripts\activate      # on Windows
# or
source .venv/bin/activate     # on Linux/Mac

# install dependencies
pip install -r requirements.txt

# parse your PDFs
python -m apps.api.nlp.parse_pdf "data/raw/<your_paper>.pdf"

# run the local web app
streamlit run apps/web/app.py

Once launched, open http://localhost:8501 in your browser.

---
